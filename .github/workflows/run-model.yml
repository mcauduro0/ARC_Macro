name: BRLUSD Daily Model Run

on:
  # Daily schedule: 6:00 AM UTC (3:00 AM BRT) â€” markets closed, fresh data
  schedule:
    - cron: '0 6 * * 1-5'  # Monday-Friday at 6:00 UTC
  
  # Manual trigger for testing
  workflow_dispatch:
    inputs:
      skip_collect:
        description: 'Skip data collection (use cached data)'
        required: false
        default: 'false'
        type: boolean

env:
  PYTHON_VERSION: '3.11'

jobs:
  run-model:
    name: Run ARC Macro Risk OS
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: server/model/requirements.txt

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r server/model/requirements.txt

      # â”€â”€ Data Cache â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # Cache CSV data files between runs. Key rotates daily so fresh
      # data is fetched once per day, but subsequent re-runs reuse it.
      # restore-keys falls back to yesterday's cache for a warm start.
      - name: Restore data cache
        id: data_cache
        uses: actions/cache@v4
        with:
          path: server/model/data
          key: model-data-${{ runner.os }}-${{ github.run_id }}
          restore-keys: |
            model-data-${{ runner.os }}-

      - name: Log cache status
        run: |
          CACHED_FILES=$(ls server/model/data/*.csv 2>/dev/null | wc -l)
          echo "Cache hit: ${{ steps.data_cache.outputs.cache-hit }}"
          echo "Cached CSV files: ${CACHED_FILES}"
          if [ "${CACHED_FILES}" -gt 0 ]; then
            echo "::notice::Restored ${CACHED_FILES} cached CSV files â€” only deltas will be fetched"
          fi

      # â”€â”€ Data Collection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Collect market data
        id: collect_data
        if: ${{ github.event.inputs.skip_collect != 'true' }}
        working-directory: server/model
        env:
          PYTHONUNBUFFERED: '1'
        run: |
          echo "::group::Data Collection"
          START_COL=$(date +%s)
          
          python3.11 -c "
          import sys, warnings
          warnings.filterwarnings('ignore')
          from data_collector import collect_all
          try:
              data = collect_all()
              n = len(data) if isinstance(data, dict) else 0
              print(f'[COLLECT] Completed: {n} series', file=sys.stderr)
          except Exception as e:
              print(f'[COLLECT] Error: {e}', file=sys.stderr)
          " 2>&1 | tee ${{ runner.temp }}/collect_stderr.log
          
          END_COL=$(date +%s)
          COL_DURATION=$((END_COL - START_COL))
          echo "::endgroup::"
          
          SERIES_COUNT=$(ls data/*.csv 2>/dev/null | wc -l)
          echo "Collection duration: ${COL_DURATION}s ($(( COL_DURATION / 60 ))m)"
          echo "CSV files: ${SERIES_COUNT}"
          echo "collect_duration=${COL_DURATION}" >> $GITHUB_OUTPUT
          echo "series_count=${SERIES_COUNT}" >> $GITHUB_OUTPUT

      # â”€â”€ Model Execution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Run ARC Macro Risk OS model
        id: run_model
        working-directory: server/model
        env:
          PYTHONUNBUFFERED: '1'
        run: |
          echo "::group::Model Execution"
          START_TIME=$(date +%s)
          
          # Always skip collection in run_model.py since we collected above
          python3.11 run_model.py --skip-collect \
            > ${{ runner.temp }}/model_output.json \
            2> >(tee ${{ runner.temp }}/model_stderr.log >&2)
          
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          echo "::endgroup::"
          
          # Validate output
          OUTPUT_SIZE=$(stat -f%z "${{ runner.temp }}/model_output.json" 2>/dev/null || stat -c%s "${{ runner.temp }}/model_output.json")
          echo "Model output size: ${OUTPUT_SIZE} bytes"
          echo "Model duration: ${DURATION} seconds ($(( DURATION / 60 )) minutes)"
          
          if [ "$OUTPUT_SIZE" -lt 1000 ]; then
            echo "::error::Model output too small (${OUTPUT_SIZE} bytes). Likely failed."
            cat ${{ runner.temp }}/model_output.json
            exit 1
          fi
          
          # Validate JSON
          python3.11 -c "import json; d=json.load(open('${{ runner.temp }}/model_output.json')); print(f'Dashboard keys: {len(d.get(\"dashboard\",{}))}'); print(f'Timeseries points: {len(d.get(\"timeseries\",[]))}')"
          
          echo "duration=${DURATION}" >> $GITHUB_OUTPUT
          echo "output_size=${OUTPUT_SIZE}" >> $GITHUB_OUTPUT

      # â”€â”€ Upload Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Upload model output to Manus dashboard
        id: upload_s3
        env:
          MANUS_WEBHOOK_SECRET: ${{ secrets.MANUS_WEBHOOK_SECRET }}
          MANUS_DASHBOARD_URL: ${{ secrets.MANUS_DASHBOARD_URL }}
        run: |
          # Compress the output for faster upload
          gzip -c ${{ runner.temp }}/model_output.json > ${{ runner.temp }}/model_output.json.gz
          
          # Upload to the dashboard via webhook endpoint
          echo "Uploading model output to Manus dashboard..."
          
          RESPONSE=$(curl -s -w "\n%{http_code}" \
            -X POST "${MANUS_DASHBOARD_URL}/api/webhook/model-result" \
            -H "Authorization: Bearer ${MANUS_WEBHOOK_SECRET}" \
            -H "Content-Type: application/json" \
            -H "Content-Encoding: gzip" \
            --data-binary @${{ runner.temp }}/model_output.json.gz \
            --max-time 120)
          
          HTTP_CODE=$(echo "$RESPONSE" | tail -1)
          BODY=$(echo "$RESPONSE" | head -n -1)
          
          echo "HTTP Status: ${HTTP_CODE}"
          echo "Response: ${BODY}"
          
          if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
            echo "Successfully uploaded to Manus dashboard"
            echo "upload_success=true" >> $GITHUB_OUTPUT
          else
            echo "::warning::Failed to upload to Manus dashboard (HTTP ${HTTP_CODE}). Saving as artifact."
            echo "upload_success=false" >> $GITHUB_OUTPUT
          fi

      # â”€â”€ Artifacts & Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Save model output as artifact (backup)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: model-output-${{ github.run_number }}
          path: |
            ${{ runner.temp }}/model_output.json
            ${{ runner.temp }}/model_stderr.log
            ${{ runner.temp }}/collect_stderr.log
          retention-days: 30

      - name: Post run summary
        if: always()
        run: |
          echo "## ðŸ¦ ARC Macro Risk OS â€” Run Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| **Data Collection** | ${{ steps.collect_data.outputs.collect_duration || 'skipped' }}s |" >> $GITHUB_STEP_SUMMARY
          echo "| **CSV Series** | ${{ steps.collect_data.outputs.series_count || 'cached' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Cache Hit** | ${{ steps.data_cache.outputs.cache-hit || 'false' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Model Duration** | ${{ steps.run_model.outputs.duration }}s ($(( ${{ steps.run_model.outputs.duration || 0 }} / 60 ))m) |" >> $GITHUB_STEP_SUMMARY
          echo "| **Output Size** | ${{ steps.run_model.outputs.output_size }} bytes |" >> $GITHUB_STEP_SUMMARY
          echo "| **Dashboard Upload** | ${{ steps.upload_s3.outputs.upload_success || 'skipped' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Trigger** | ${{ github.event_name }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Run** | #${{ github.run_number }} |" >> $GITHUB_STEP_SUMMARY
